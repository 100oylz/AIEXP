[
  "epoch 1 saved! Because of Loss",
  "epoch 1 saved! Because of TrainAcc",
  "epoch 1 saved! Because of TestAcc",
  "epoch:1 -> loss:0.02136869728565216 , train_acc:0.8816833333333334,test_acc:0.8397",
  "epoch:2 -> loss:0.022119702771306038 , train_acc:0.8804333333333333,test_acc:0.8388",
  "epoch:3 -> loss:0.022552480921149254 , train_acc:0.8803666666666666,test_acc:0.8394",
  "epoch:4 -> loss:0.022855665534734726 , train_acc:0.8802666666666666,test_acc:0.8388",
  "epoch:5 -> loss:0.02294483594596386 , train_acc:0.8804666666666666,test_acc:0.8343",
  "epoch:6 -> loss:0.02319497987627983 , train_acc:0.8798,test_acc:0.8343",
  "epoch:7 -> loss:0.02331719361245632 , train_acc:0.8805,test_acc:0.8374",
  "epoch:8 -> loss:0.02340054325759411 , train_acc:0.8799666666666667,test_acc:0.838",
  "epoch:9 -> loss:0.02358708158135414 , train_acc:0.8797333333333334,test_acc:0.8346",
  "epoch:10 -> loss:0.023662351071834564 , train_acc:0.8794833333333333,test_acc:0.8357",
  "epoch:11 -> loss:0.02361481636762619 , train_acc:0.8798666666666667,test_acc:0.8373",
  "epoch:12 -> loss:0.023765936493873596 , train_acc:0.8801166666666667,test_acc:0.8389",
  "epoch:13 -> loss:0.023514412343502045 , train_acc:0.8800333333333333,test_acc:0.8347",
  "epoch:14 -> loss:0.023818710818886757 , train_acc:0.8801166666666667,test_acc:0.8345",
  "epoch:15 -> loss:0.023861443623900414 , train_acc:0.87925,test_acc:0.8358",
  "epoch:16 -> loss:0.02394973859190941 , train_acc:0.8790666666666667,test_acc:0.836",
  "epoch:17 -> loss:0.023995038121938705 , train_acc:0.8796666666666667,test_acc:0.8335",
  "epoch:18 -> loss:0.02424292080104351 , train_acc:0.87865,test_acc:0.8346",
  "epoch:19 -> loss:0.024069061502814293 , train_acc:0.87965,test_acc:0.8305",
  "epoch:20 -> loss:0.024143360555171967 , train_acc:0.8783666666666666,test_acc:0.833",
  "epoch:21 -> loss:0.02417161501944065 , train_acc:0.8791666666666667,test_acc:0.8347",
  "epoch:22 -> loss:0.02405889518558979 , train_acc:0.8787,test_acc:0.834",
  "epoch:23 -> loss:0.024283520877361298 , train_acc:0.8780333333333333,test_acc:0.8336",
  "epoch:24 -> loss:0.024328628554940224 , train_acc:0.8796,test_acc:0.8346",
  "epoch:25 -> loss:0.024325132369995117 , train_acc:0.8783333333333333,test_acc:0.8336",
  "epoch:26 -> loss:0.024471009150147438 , train_acc:0.87905,test_acc:0.8346",
  "epoch:27 -> loss:0.024281613528728485 , train_acc:0.8789666666666667,test_acc:0.8301",
  "epoch:28 -> loss:0.024393578991293907 , train_acc:0.8787,test_acc:0.8301",
  "epoch:29 -> loss:0.024337299168109894 , train_acc:0.8782333333333333,test_acc:0.8327",
  "epoch:30 -> loss:0.024382680654525757 , train_acc:0.8790666666666667,test_acc:0.8339",
  "epoch:31 -> loss:0.024178337305784225 , train_acc:0.8787666666666667,test_acc:0.8338",
  "epoch:32 -> loss:0.024338753893971443 , train_acc:0.8793,test_acc:0.8303",
  "epoch:33 -> loss:0.02435343712568283 , train_acc:0.8790166666666667,test_acc:0.8317",
  "epoch:34 -> loss:0.024217016994953156 , train_acc:0.8790333333333333,test_acc:0.8311",
  "epoch:35 -> loss:0.024249780923128128 , train_acc:0.8792166666666666,test_acc:0.831",
  "epoch:36 -> loss:0.02418108843266964 , train_acc:0.8784166666666666,test_acc:0.8335",
  "epoch:37 -> loss:0.02419891394674778 , train_acc:0.8782166666666666,test_acc:0.8335",
  "epoch:38 -> loss:0.024217963218688965 , train_acc:0.8790333333333333,test_acc:0.8301",
  "epoch:39 -> loss:0.024060411378741264 , train_acc:0.8797833333333334,test_acc:0.8329",
  "epoch:40 -> loss:0.02420976012945175 , train_acc:0.87865,test_acc:0.8324",
  "epoch:41 -> loss:0.02416043169796467 , train_acc:0.8796,test_acc:0.8301",
  "epoch:42 -> loss:0.024128546938300133 , train_acc:0.87955,test_acc:0.8313",
  "epoch:43 -> loss:0.024122141301631927 , train_acc:0.8788833333333333,test_acc:0.8339",
  "epoch:44 -> loss:0.024175414815545082 , train_acc:0.87895,test_acc:0.8255",
  "epoch:45 -> loss:0.024116791784763336 , train_acc:0.8798833333333334,test_acc:0.8287",
  "epoch:46 -> loss:0.024129418656229973 , train_acc:0.8794333333333333,test_acc:0.8325",
  "epoch:47 -> loss:0.02418319322168827 , train_acc:0.8804666666666666,test_acc:0.8324",
  "epoch:48 -> loss:0.024107487872242928 , train_acc:0.8794666666666666,test_acc:0.8306",
  "epoch:49 -> loss:0.02410091646015644 , train_acc:0.8795333333333333,test_acc:0.8352",
  "epoch:50 -> loss:0.02398991398513317 , train_acc:0.8796666666666667,test_acc:0.832",
  "The Best Loss:0.02136869728565216!\nThe Best Train Accuracy:0.8816833333333334!\nThe Best Test Accuract:0.8397!"
]