[
  "epoch 1 saved! Because of Loss",
  "epoch 1 saved! Because of TrainAcc",
  "epoch 1 saved! Because of TestAcc",
  "epoch:1 -> loss:0.04503816366195679 , train_acc:0.7585666666666666,test_acc:0.7536",
  "epoch 2 saved! Because of Loss",
  "epoch 2 saved! Because of TrainAcc",
  "epoch 2 saved! Because of TestAcc",
  "epoch:2 -> loss:0.038217492401599884 , train_acc:0.7918333333333333,test_acc:0.7662",
  "epoch 3 saved! Because of Loss",
  "epoch 3 saved! Because of TrainAcc",
  "epoch 3 saved! Because of TestAcc",
  "epoch:3 -> loss:0.03730488196015358 , train_acc:0.79855,test_acc:0.808",
  "epoch 4 saved! Because of Loss",
  "epoch 4 saved! Because of TrainAcc",
  "epoch 4 saved! Because of TestAcc",
  "epoch:4 -> loss:0.03571176901459694 , train_acc:0.8071333333333334,test_acc:0.8164",
  "epoch 5 saved! Because of Loss",
  "epoch 5 saved! Because of TrainAcc",
  "epoch:5 -> loss:0.03433602303266525 , train_acc:0.8158166666666666,test_acc:0.7841",
  "epoch:6 -> loss:0.034701671451330185 , train_acc:0.8151833333333334,test_acc:0.7406",
  "epoch:7 -> loss:0.03572861850261688 , train_acc:0.8136833333333333,test_acc:0.8047",
  "epoch 8 saved! Because of TestAcc",
  "epoch:8 -> loss:0.03454611077904701 , train_acc:0.8149833333333333,test_acc:0.8207",
  "epoch 9 saved! Because of Loss",
  "epoch 9 saved! Because of TrainAcc",
  "epoch:9 -> loss:0.03265051916241646 , train_acc:0.8264833333333333,test_acc:0.8028",
  "epoch 10 saved! Because of Loss",
  "epoch 10 saved! Because of TestAcc",
  "epoch:10 -> loss:0.03261076658964157 , train_acc:0.8257833333333333,test_acc:0.842",
  "epoch 11 saved! Because of Loss",
  "epoch:11 -> loss:0.03254622593522072 , train_acc:0.8264666666666667,test_acc:0.8258",
  "epoch 12 saved! Because of Loss",
  "epoch 12 saved! Because of TrainAcc",
  "epoch:12 -> loss:0.03207463026046753 , train_acc:0.82685,test_acc:0.8113",
  "epoch 13 saved! Because of TrainAcc",
  "epoch:13 -> loss:0.03257080167531967 , train_acc:0.8312333333333334,test_acc:0.8109",
  "epoch:14 -> loss:0.032621078193187714 , train_acc:0.8263666666666667,test_acc:0.8162",
  "epoch:15 -> loss:0.03275902569293976 , train_acc:0.8257166666666667,test_acc:0.8225",
  "epoch:16 -> loss:0.03395300731062889 , train_acc:0.8245666666666667,test_acc:0.8097",
  "epoch:17 -> loss:0.03308688476681709 , train_acc:0.8291,test_acc:0.8148",
  "epoch:18 -> loss:0.033336758613586426 , train_acc:0.8255666666666667,test_acc:0.7206",
  "epoch:19 -> loss:0.03390655666589737 , train_acc:0.82425,test_acc:0.8218",
  "epoch:20 -> loss:0.034644100815057755 , train_acc:0.8248666666666666,test_acc:0.7796",
  "epoch:21 -> loss:0.03429938480257988 , train_acc:0.8239666666666666,test_acc:0.7871",
  "epoch:22 -> loss:0.03904353454709053 , train_acc:0.8074166666666667,test_acc:0.8167",
  "epoch:23 -> loss:0.036261122673749924 , train_acc:0.8163333333333334,test_acc:0.7899",
  "epoch:24 -> loss:0.04282793030142784 , train_acc:0.7958333333333333,test_acc:0.7955",
  "epoch:25 -> loss:0.03789769858121872 , train_acc:0.8142666666666667,test_acc:0.8057",
  "epoch:26 -> loss:0.03545583412051201 , train_acc:0.8191833333333334,test_acc:0.7953",
  "epoch:27 -> loss:0.033907368779182434 , train_acc:0.8257833333333333,test_acc:0.7637",
  "epoch:28 -> loss:0.035635147243738174 , train_acc:0.8210833333333334,test_acc:0.8115",
  "epoch:29 -> loss:0.03298459202051163 , train_acc:0.8306333333333333,test_acc:0.8089",
  "epoch 30 saved! Because of TrainAcc",
  "epoch:30 -> loss:0.03275912627577782 , train_acc:0.8332833333333334,test_acc:0.7669",
  "epoch:31 -> loss:0.03356822952628136 , train_acc:0.8294166666666667,test_acc:0.8152",
  "epoch:32 -> loss:0.03407642990350723 , train_acc:0.8270833333333333,test_acc:0.7483",
  "epoch:33 -> loss:0.03308597207069397 , train_acc:0.8320166666666666,test_acc:0.8067",
  "epoch:34 -> loss:0.03324725851416588 , train_acc:0.83075,test_acc:0.7797",
  "epoch:35 -> loss:0.033274583518505096 , train_acc:0.8307666666666667,test_acc:0.7926",
  "epoch:36 -> loss:0.03223545849323273 , train_acc:0.8317166666666667,test_acc:0.8146",
  "epoch:37 -> loss:0.03388841077685356 , train_acc:0.8309,test_acc:0.7619",
  "epoch:38 -> loss:0.03415670618414879 , train_acc:0.8268833333333333,test_acc:0.8087",
  "epoch:39 -> loss:0.0362699031829834 , train_acc:0.8202333333333334,test_acc:0.8156",
  "epoch:40 -> loss:0.036997612565755844 , train_acc:0.82185,test_acc:0.7766",
  "epoch:41 -> loss:0.036264076828956604 , train_acc:0.8237833333333333,test_acc:0.7681",
  "epoch:42 -> loss:0.03677527233958244 , train_acc:0.8191833333333334,test_acc:0.8141",
  "epoch:43 -> loss:0.03535057231783867 , train_acc:0.8255666666666667,test_acc:0.8098",
  "epoch:44 -> loss:0.03564661368727684 , train_acc:0.8243333333333334,test_acc:0.8158",
  "epoch:45 -> loss:0.03538380563259125 , train_acc:0.8271166666666666,test_acc:0.8234",
  "epoch:46 -> loss:0.03376663476228714 , train_acc:0.8332333333333334,test_acc:0.7799",
  "epoch:47 -> loss:0.035208601504564285 , train_acc:0.8266333333333333,test_acc:0.8266",
  "epoch:48 -> loss:0.0384221151471138 , train_acc:0.8175333333333333,test_acc:0.8213",
  "epoch:49 -> loss:0.03536305949091911 , train_acc:0.8243,test_acc:0.8301",
  "epoch:50 -> loss:0.03776763379573822 , train_acc:0.8196166666666667,test_acc:0.807",
  "The Best Loss:0.03207463026046753!\nThe Best Train Accuracy:0.8332833333333334!\nThe Best Test Accuract:0.842!"
]